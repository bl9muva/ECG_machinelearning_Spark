{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Neural Network (may take an hour to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col,sum, isnan\n",
    "from scipy.signal import find_peaks\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "import matplotlib.pyplot as plt \n",
    "import biosppy\n",
    "from biosppy.signals import ecg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data(df, sampling_rate, path):\n",
    "    if sampling_rate == 100:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_lr]\n",
    "    else:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_hr]\n",
    "    data = np.array([signal for signal, meta in data])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1/\"\n",
    "sampling_rate=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_raw_data(Y, sampling_rate, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.read_csv('ECG_features.csv', index_col='ecg_id')\n",
    "Y.reset_index(drop=False, inplace=True)\n",
    "Y.index += 1\n",
    "Y.head()\n",
    "len(Y.columns)\n",
    "Y = Y.replace(float(\"nan\"), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y['NORM'] = Y['NORM'].astype(bool).astype(int)\n",
    "Y['STTC'] = Y['STTC'].astype(bool).astype(int)\n",
    "Y['MI'] = Y['MI'].astype(bool).astype(int)\n",
    "Y['HYP'] = Y['HYP'].astype(bool).astype(int)\n",
    "Y['CD'] = Y['CD'].astype(bool).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SQLContext\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"mllib_classifier\") \\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sql = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySchema = StructType([ \n",
    "    StructField(\"ecg_id\", IntegerType(), True)\\\n",
    "    ,StructField(\"patient_id\", FloatType(), True)\\\n",
    "    ,StructField(\"age\", FloatType(), True)\\\n",
    "    ,StructField(\"sex\", IntegerType(), True)\\\n",
    "    ,StructField(\"height\", FloatType(), True)\\\n",
    "    ,StructField(\"weight\", FloatType(), True)\\\n",
    "    ,StructField(\"nurse\", FloatType(), True)\\\n",
    "    ,StructField(\"site\", StringType(), True)\\\n",
    "    ,StructField(\"device\", StringType(), True)\\\n",
    "    ,StructField(\"recording_date\", StringType(), True)\\\n",
    "    ,StructField(\"report\", StringType(), True)\\\n",
    "    ,StructField(\"scp_codes\", StringType(), True)\\\n",
    "    ,StructField(\"heart_axis\", StringType(), True)\\\n",
    "    ,StructField(\"infarction_stadium1\", StringType(), True)\\\n",
    "    ,StructField(\"infarction_stadium2\", StringType(), True)\\\n",
    "    ,StructField(\"validated_by\", FloatType(), True)\\\n",
    "    ,StructField(\"second_opinion\", StringType(), True)\\\n",
    "    ,StructField(\"initial_autogenerated_report\", StringType(), True)\\\n",
    "    ,StructField(\"validated_by_human\", StringType(), True)\\\n",
    "    ,StructField(\"baseline_drift\", StringType(), True)\\\n",
    "    ,StructField(\"static_noise\", StringType(), True)\\\n",
    "    ,StructField(\"burst_noise\", StringType(), True)\\\n",
    "    ,StructField(\"electrodes_problems\", StringType(), True)\\\n",
    "    ,StructField(\"extra_beats\", StringType(), True)\\\n",
    "    ,StructField(\"pacemaker\", StringType(), True)\\\n",
    "    ,StructField(\"strat_fold\", StringType(), True)\\\n",
    "    ,StructField(\"filename_lr\", StringType(), True)\\\n",
    "    ,StructField(\"filename_hr\", StringType(), True)\\\n",
    "    ,StructField(\"diagnostic_superclass\", StringType(), True)\\\n",
    "    ,StructField(\"NORM\", IntegerType(), True)\\\n",
    "    ,StructField(\"MI\", IntegerType(), True)\\\n",
    "    ,StructField(\"STTC\", IntegerType(), True)\\\n",
    "    ,StructField(\"HYP\", IntegerType(), True)\\\n",
    "    ,StructField(\"CD\", IntegerType(), True)\\\n",
    "    ,StructField(\"bpm\", FloatType(), True)\\\n",
    "    ,StructField(\"bif\", FloatType(), True)\\\n",
    "    ,StructField(\"bif2\", FloatType(), True)\\\n",
    "    ,StructField(\"TRinterval\", FloatType(), True)\\\n",
    "    ,StructField(\"TRratio\", FloatType(), True)\\\n",
    "    ,StructField(\"PRinterval\", FloatType(), True)\\\n",
    "    ,StructField(\"PRratio\", FloatType(), True)\\\n",
    "    ,StructField(\"QRinterval\", FloatType(), True)\\\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(Y,schema=mySchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_keep = [\"diagnostic_superclass\",\"age\",\"sex\", \"bpm\", \"bif\", \"bif2\", \"TRinterval\", \"TRratio\",\"PRinterval\",\"PRratio\",\"QRinterval\",\"NORM\", \"MI\",\"STTC\",\"HYP\",\"CD\",\n",
    "               \"strat_fold\"]\n",
    "\n",
    "# subset the dataframe on these predictors\n",
    "df2 = df[vars_to_keep]\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"age\",\"sex\", \"bpm\", \"bif\", \"bif2\", \"TRinterval\", \"TRratio\",\"PRinterval\",\"PRratio\",\"QRinterval\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "df2 = assembler.transform(df2)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.mllib.regression as reg\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Initialize the `standardScaler`\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\", \n",
    "                                withStd=True, withMean=False)\n",
    "\n",
    "# Fit the DataFrame to the scaler; this computes the mean, standard deviation of each feature\n",
    "scaler = standardScaler.fit(df2)\n",
    "\n",
    "# Transform the data in `df2` with the scaler\n",
    "scaled_df = scaler.transform(df2)\n",
    "\n",
    "scaled_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "scaled_df2 = scaled_df.withColumn(\"Target\", F.when(col('NORM') == 1, 1).otherwise(0))\n",
    "\n",
    "target = np.array(scaled_df2.select('Target').collect())\n",
    "\n",
    "features = np.array(scaled_df2.select('features').collect())\n",
    "\n",
    "features2 = np.array(scaled_df2.select('features_scaled').collect())\n",
    "\n",
    "df3 = df[vars_to_keep]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"bpm\", \"bif\", \"bif2\", \"TRinterval\", \"TRratio\",\"PRinterval\",\"PRratio\",\"QRinterval\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "df3 = assembler.transform(df3)\n",
    "\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\", \n",
    "                                withStd=True, withMean=False)\n",
    "\n",
    "# Fit the DataFrame to the scaler; this computes the mean, standard deviation of each feature\n",
    "scaler = standardScaler.fit(df3)\n",
    "\n",
    "# Transform the data in `df2` with the scaler\n",
    "scaled_df3 = scaler.transform(df3)\n",
    "\n",
    "scaled_df3 = scaled_df3.withColumn(\"Target\", F.when(col('NORM') == 1, 1).otherwise(0))\n",
    "\n",
    "target3 = np.array(scaled_df3.select('Target').collect())\n",
    "features3 = np.array(scaled_df3.select('features_scaled').collect())\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "x_train, x_test = features2[0:16000], features2[16000:21837]\n",
    "\n",
    "x_train2, x_test2 = features[0:16000], features[16000:21837]\n",
    "\n",
    "y_train, y_test = target[0:16000], target[16000:21837]\n",
    "\n",
    "x_train3, x_test3 = features3[0:16000], features3[16000:21837]\n",
    "\n",
    "y_train3, y_test3 = target3[0:16000], target3[16000:21837]\n",
    "\n",
    "def neural_net(x_train, y_train, x_test, y_test, epochs, batch_size):\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Flatten(input_shape=x_train.shape[1:3]),\n",
    "      tf.keras.layers.Dense(256, activation='relu'),\n",
    "      tf.keras.layers.Dropout(0.1),\n",
    "      tf.keras.layers.Dense(256, activation='relu'),\n",
    "      tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    return model.evaluate(x_test, y_test)\n",
    "\n",
    "neural_net(x_train, y_train, x_test, y_test, 1000, 256) #Test Accuracy: 0.5988\n",
    "\n",
    "neural_net(x_train, y_train, x_test, y_test, 1000, 128) #Test Accuracy: 0.6087\n",
    "\n",
    "neural_net(x_train2, y_train, x_test2, y_test, 1000, 256) #Test Accuracy: 0.6356\n",
    "\n",
    "neural_net(x_train3, y_train3, x_test3, y_test3, 1000, 128) #Test Accuracy: 0.5220\n",
    "\n",
    "x_train4, y_train4 = X[0:16000], target[0:16000]\n",
    "\n",
    "x_test4, y_test4 = X[16000:21837], target[16000:21837]\n",
    "\n",
    "neural_net(x_train4, y_train4, x_test4, y_test4, 1000, 128) #Test Accuracy: 0.5102"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5559",
   "language": "python",
   "name": "ds5559"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
