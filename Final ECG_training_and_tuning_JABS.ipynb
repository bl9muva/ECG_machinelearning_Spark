{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group JABS \n",
    "## Notebook for Pipeline Creation and Tuning\n",
    "\n",
    "## __Please note that there is some repetition of code from the previous notebook so that this file may be run independently.__\n",
    "\n",
    "\n",
    "The purpose of this notebook is to showcase pipeline creation and more robust model creation and evaluation. Please see the section labelled \"Pipeline and Cross Validation\" for new code (compared to the previous notebook).\n",
    "\n",
    "Please also note that the benchmark model was create in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with zipfile.ZipFile(\"ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1.zip\", \"r\") as zip_ref:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import wfdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col,sum, isnan\n",
    "from scipy.signal import find_peaks\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "import matplotlib.pyplot as plt \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.read_csv('ECG_features.csv', index_col='ecg_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resetting the index so we can include the ecg_id column when converting to a Spark datafram\n",
    "Y.reset_index(drop=False, inplace=True)\n",
    "Y.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ecg_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>nurse</th>\n",
       "      <th>site</th>\n",
       "      <th>device</th>\n",
       "      <th>recording_date</th>\n",
       "      <th>...</th>\n",
       "      <th>HYP</th>\n",
       "      <th>CD</th>\n",
       "      <th>bpm</th>\n",
       "      <th>bif</th>\n",
       "      <th>bif2</th>\n",
       "      <th>TRinterval</th>\n",
       "      <th>TRratio</th>\n",
       "      <th>PRinterval</th>\n",
       "      <th>PRratio</th>\n",
       "      <th>QRinterval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19434</th>\n",
       "      <td>19434</td>\n",
       "      <td>9321.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CS100    3</td>\n",
       "      <td>1998-07-26 12:52:32</td>\n",
       "      <td>...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.257611</td>\n",
       "      <td>0.046838</td>\n",
       "      <td>0.017525</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>0.268318</td>\n",
       "      <td>327.000000</td>\n",
       "      <td>0.101439</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21588</th>\n",
       "      <td>21588</td>\n",
       "      <td>17435.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CS100    3</td>\n",
       "      <td>2000-11-26 13:34:18</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.257669</td>\n",
       "      <td>0.077301</td>\n",
       "      <td>0.024478</td>\n",
       "      <td>766.666667</td>\n",
       "      <td>0.193526</td>\n",
       "      <td>148.888889</td>\n",
       "      <td>0.193526</td>\n",
       "      <td>71.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21377</th>\n",
       "      <td>21377</td>\n",
       "      <td>18433.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CS100    3</td>\n",
       "      <td>2000-09-20 07:55:14</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>129.032258</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.187788</td>\n",
       "      <td>163.684211</td>\n",
       "      <td>0.124978</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>0.094251</td>\n",
       "      <td>153.157895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17884</th>\n",
       "      <td>17884</td>\n",
       "      <td>13588.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CS-12   E</td>\n",
       "      <td>1997-05-11 09:42:14</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.783354</td>\n",
       "      <td>0.080783</td>\n",
       "      <td>0.025084</td>\n",
       "      <td>407.272727</td>\n",
       "      <td>0.138047</td>\n",
       "      <td>139.090909</td>\n",
       "      <td>0.076199</td>\n",
       "      <td>61.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8254</th>\n",
       "      <td>8254</td>\n",
       "      <td>12080.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CS-12</td>\n",
       "      <td>1992-06-22 17:28:17</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.146497</td>\n",
       "      <td>0.621656</td>\n",
       "      <td>0.162294</td>\n",
       "      <td>435.000000</td>\n",
       "      <td>0.231547</td>\n",
       "      <td>162.500000</td>\n",
       "      <td>0.187036</td>\n",
       "      <td>62.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13145</th>\n",
       "      <td>13145</td>\n",
       "      <td>18250.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CS-12</td>\n",
       "      <td>1994-10-06 18:16:39</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68.354430</td>\n",
       "      <td>0.045570</td>\n",
       "      <td>0.014977</td>\n",
       "      <td>263.333333</td>\n",
       "      <td>0.211568</td>\n",
       "      <td>138.888889</td>\n",
       "      <td>0.041927</td>\n",
       "      <td>37.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10990</th>\n",
       "      <td>10990</td>\n",
       "      <td>1122.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1</td>\n",
       "      <td>158.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AT-6 C 5.5</td>\n",
       "      <td>1993-09-11 10:59:06</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>126.666667</td>\n",
       "      <td>1.330000</td>\n",
       "      <td>0.320694</td>\n",
       "      <td>209.473684</td>\n",
       "      <td>0.073049</td>\n",
       "      <td>171.052632</td>\n",
       "      <td>0.050314</td>\n",
       "      <td>53.684211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17862</th>\n",
       "      <td>17862</td>\n",
       "      <td>1895.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1</td>\n",
       "      <td>174.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AT-6     6</td>\n",
       "      <td>1997-05-05 14:14:00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>56.404230</td>\n",
       "      <td>0.169213</td>\n",
       "      <td>0.062700</td>\n",
       "      <td>501.250000</td>\n",
       "      <td>0.181249</td>\n",
       "      <td>187.500000</td>\n",
       "      <td>0.146324</td>\n",
       "      <td>68.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11500</th>\n",
       "      <td>11500</td>\n",
       "      <td>13887.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CS-12</td>\n",
       "      <td>1993-12-11 15:48:16</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>96.109840</td>\n",
       "      <td>0.048055</td>\n",
       "      <td>0.013145</td>\n",
       "      <td>241.428571</td>\n",
       "      <td>0.182264</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>0.127693</td>\n",
       "      <td>43.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15963</th>\n",
       "      <td>15963</td>\n",
       "      <td>12535.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CS-12   E</td>\n",
       "      <td>1996-04-18 09:00:13</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.191781</td>\n",
       "      <td>0.534247</td>\n",
       "      <td>0.176672</td>\n",
       "      <td>245.833333</td>\n",
       "      <td>0.138094</td>\n",
       "      <td>321.666667</td>\n",
       "      <td>0.098816</td>\n",
       "      <td>179.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ecg_id  patient_id   age  sex  height  weight  nurse  site      device  \\\n",
       "19434   19434      9321.0  69.0    0     NaN     NaN    0.0   0.0  CS100    3   \n",
       "21588   21588     17435.0  67.0    0     NaN     NaN    0.0   0.0  CS100    3   \n",
       "21377   21377     18433.0  82.0    0     NaN     NaN    0.0   0.0  CS100    3   \n",
       "17884   17884     13588.0  33.0    1     NaN    50.0    0.0   0.0   CS-12   E   \n",
       "8254     8254     12080.0  60.0    1     NaN     NaN    1.0   2.0       CS-12   \n",
       "13145   13145     18250.0  67.0    1     NaN     NaN    1.0   2.0       CS-12   \n",
       "10990   10990      1122.0  44.0    1   158.0     NaN   10.0   1.0  AT-6 C 5.5   \n",
       "17862   17862      1895.0  76.0    1   174.0    73.0    6.0   1.0  AT-6     6   \n",
       "11500   11500     13887.0  90.0    1     NaN     NaN    1.0   2.0       CS-12   \n",
       "15963   15963     12535.0  34.0    1     NaN    59.0    0.0   0.0   CS-12   E   \n",
       "\n",
       "            recording_date  ...   HYP     CD         bpm       bif      bif2  \\\n",
       "19434  1998-07-26 12:52:32  ...  50.0    NaN   70.257611  0.046838  0.017525   \n",
       "21588  2000-11-26 13:34:18  ...   NaN    NaN   66.257669  0.077301  0.024478   \n",
       "21377  2000-09-20 07:55:14  ...   NaN    NaN  129.032258  0.666667  0.187788   \n",
       "17884  1997-05-11 09:42:14  ...   NaN    NaN   80.783354  0.080783  0.025084   \n",
       "8254   1992-06-22 17:28:17  ...   NaN    NaN   61.146497  0.621656  0.162294   \n",
       "13145  1994-10-06 18:16:39  ...   NaN    NaN   68.354430  0.045570  0.014977   \n",
       "10990  1993-09-11 10:59:06  ...   NaN    NaN  126.666667  1.330000  0.320694   \n",
       "17862  1997-05-05 14:14:00  ...   NaN  100.0   56.404230  0.169213  0.062700   \n",
       "11500  1993-12-11 15:48:16  ...   NaN  100.0   96.109840  0.048055  0.013145   \n",
       "15963  1996-04-18 09:00:13  ...   NaN    NaN   82.191781  0.534247  0.176672   \n",
       "\n",
       "       TRinterval   TRratio  PRinterval   PRratio  QRinterval  \n",
       "19434  266.000000  0.268318  327.000000  0.101439   43.000000  \n",
       "21588  766.666667  0.193526  148.888889  0.193526   71.111111  \n",
       "21377  163.684211  0.124978  210.000000  0.094251  153.157895  \n",
       "17884  407.272727  0.138047  139.090909  0.076199   61.818182  \n",
       "8254   435.000000  0.231547  162.500000  0.187036   62.500000  \n",
       "13145  263.333333  0.211568  138.888889  0.041927   37.777778  \n",
       "10990  209.473684  0.073049  171.052632  0.050314   53.684211  \n",
       "17862  501.250000  0.181249  187.500000  0.146324   68.750000  \n",
       "11500  241.428571  0.182264  170.000000  0.127693   43.571429  \n",
       "15963  245.833333  0.138094  321.666667  0.098816  179.166667  \n",
       "\n",
       "[10 rows x 42 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y.replace(float(\"nan\"), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y['NORM'] = Y['NORM'].astype(bool).astype(int)\n",
    "Y['STTC'] = Y['STTC'].astype(bool).astype(int)\n",
    "Y['MI'] = Y['MI'].astype(bool).astype(int)\n",
    "Y['HYP'] = Y['HYP'].astype(bool).astype(int)\n",
    "Y['CD'] = Y['CD'].astype(bool).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting to Spark df\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SQLContext\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"mllib_classifier\") \\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sql = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_df = sql.createDataFrame(Y)\n",
    "from pyspark.sql.types import *\n",
    "mySchema = StructType([ \n",
    "    StructField(\"ecg_id\", IntegerType(), True)\\\n",
    "    ,StructField(\"patient_id\", FloatType(), True)\\\n",
    "    ,StructField(\"age\", FloatType(), True)\\\n",
    "    ,StructField(\"sex\", IntegerType(), True)\\\n",
    "    ,StructField(\"height\", FloatType(), True)\\\n",
    "    ,StructField(\"weight\", FloatType(), True)\\\n",
    "    ,StructField(\"nurse\", FloatType(), True)\\\n",
    "    ,StructField(\"site\", StringType(), True)\\\n",
    "    ,StructField(\"device\", StringType(), True)\\\n",
    "    ,StructField(\"recording_date\", StringType(), True)\\\n",
    "    ,StructField(\"report\", StringType(), True)\\\n",
    "    ,StructField(\"scp_codes\", StringType(), True)\\\n",
    "    ,StructField(\"heart_axis\", StringType(), True)\\\n",
    "    ,StructField(\"infarction_stadium1\", StringType(), True)\\\n",
    "    ,StructField(\"infarction_stadium2\", StringType(), True)\\\n",
    "    ,StructField(\"validated_by\", FloatType(), True)\\\n",
    "    ,StructField(\"second_opinion\", StringType(), True)\\\n",
    "    ,StructField(\"initial_autogenerated_report\", StringType(), True)\\\n",
    "    ,StructField(\"validated_by_human\", StringType(), True)\\\n",
    "    ,StructField(\"baseline_drift\", StringType(), True)\\\n",
    "    ,StructField(\"static_noise\", StringType(), True)\\\n",
    "    ,StructField(\"burst_noise\", StringType(), True)\\\n",
    "    ,StructField(\"electrodes_problems\", StringType(), True)\\\n",
    "    ,StructField(\"extra_beats\", StringType(), True)\\\n",
    "    ,StructField(\"pacemaker\", StringType(), True)\\\n",
    "    ,StructField(\"strat_fold\", StringType(), True)\\\n",
    "    ,StructField(\"filename_lr\", StringType(), True)\\\n",
    "    ,StructField(\"filename_hr\", StringType(), True)\\\n",
    "    ,StructField(\"diagnostic_superclass\", StringType(), True)\\\n",
    "    ,StructField(\"NORM\", IntegerType(), True)\\\n",
    "    ,StructField(\"MI\", IntegerType(), True)\\\n",
    "    ,StructField(\"STTC\", IntegerType(), True)\\\n",
    "    ,StructField(\"HYP\", IntegerType(), True)\\\n",
    "    ,StructField(\"CD\", IntegerType(), True)\\\n",
    "    ,StructField(\"bpm\", FloatType(), True)\\\n",
    "    ,StructField(\"bif\", FloatType(), True)\\\n",
    "    ,StructField(\"bif2\", FloatType(), True)\\\n",
    "    ,StructField(\"TRinterval\", FloatType(), True)\\\n",
    "    ,StructField(\"TRratio\", FloatType(), True)\\\n",
    "    ,StructField(\"PRinterval\", FloatType(), True)\\\n",
    "    ,StructField(\"PRratio\", FloatType(), True)\\\n",
    "    ,StructField(\"QRinterval\", FloatType(), True)\\\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(Y,schema=mySchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts of MI:  5486\n"
     ]
    }
   ],
   "source": [
    "print('counts of MI: ', df.filter(df['MI']==1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21837"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Statistical summary of response variable__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts of NORM:  9528\n",
      "counts of MI:  5486\n",
      "counts of STTC:  5123\n",
      "counts of CD:  4907\n",
      "counts of HYP:  2655\n"
     ]
    }
   ],
   "source": [
    "#these numbers match the numbers on the physionet website \n",
    "print('counts of NORM: ', df.filter(df['NORM']!=0).count())\n",
    "print('counts of MI: ', df.filter(df['MI']!=0).count())\n",
    "print('counts of STTC: ', df.filter(df['STTC']!=0).count())\n",
    "print('counts of CD: ', df.filter(df['CD']!=0).count())\n",
    "print('counts of HYP: ', df.filter(df['HYP']!=0).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|strat_fold|count|\n",
      "+----------+-----+\n",
      "|         7| 2178|\n",
      "|         3| 2194|\n",
      "|         8| 2179|\n",
      "|         5| 2176|\n",
      "|         6| 2178|\n",
      "|         9| 2193|\n",
      "|         1| 2177|\n",
      "|        10| 2203|\n",
      "|         4| 2175|\n",
      "|         2| 2184|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('strat_fold').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep only columns we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----+---+---------+-----------+-----------+----------+----------+----------+-----------+----------+----+---+----+---+---+----------+\n",
      "|diagnostic_superclass| age|sex|      bpm|        bif|       bif2|TRinterval|   TRratio|PRinterval|    PRratio|QRinterval|NORM| MI|STTC|HYP| CD|strat_fold|\n",
      "+---------------------+----+---+---------+-----------+-----------+----------+----------+----------+-----------+----------+----+---+----+---+---+----------+\n",
      "|      {'NORM': 100.0}|56.0|  1|47.244095| 0.16535433|0.058571953|     285.0|0.22329463| 151.66667| 0.10876027|      45.0|   1|  0|   0|  0|  0|         3|\n",
      "|       {'NORM': 80.0}|19.0|  0| 63.75443| 0.07438017|0.022087706|  478.8889|0.22907318| 168.88889| 0.17119157|  61.11111|   1|  0|   0|  0|  0|         2|\n",
      "|      {'NORM': 100.0}|37.0|  1| 74.40812| 0.14881623|0.045572452| 231.81818|0.25935754| 204.54546| 0.09707572| 75.454544|   1|  0|   0|  0|  0|         5|\n",
      "|      {'NORM': 100.0}|24.0|  0| 66.29834| 0.16574585| 0.05418873|     267.0|0.21640064|     138.0| 0.10672184|      47.0|   1|  0|   0|  0|  0|         3|\n",
      "|      {'NORM': 100.0}|19.0|  1| 83.14088| 0.15242495|0.049262654|  568.3333|0.15125224| 163.33333| 0.15125224| 74.166664|   1|  0|   0|  0|  0|         4|\n",
      "|      {'NORM': 100.0}|18.0|  1|61.997704|  0.0413318|0.013584569| 68.888885|0.12439417| 207.77777|0.044792965|     160.0|   1|  0|   0|  0|  0|         4|\n",
      "|      {'NORM': 100.0}|54.0|  0| 73.74302|0.061452515| 0.01679698| 248.18182|0.30723533| 182.72728| 0.18345565|      50.0|   1|  0|   0|  0|  0|         7|\n",
      "|         {'MI': 35.0}|48.0|  0|60.836502| 0.11153359| 0.03546532|     262.5|0.31097305|     170.0| 0.13649388|     46.25|   0|  1|   0|  0|  0|         9|\n",
      "|      {'NORM': 100.0}|55.0|  0| 62.93706| 0.17832167|0.060963623| 234.44444|0.19047683|     180.0| 0.08553792| 68.888885|   1|  0|   0|  0|  0|        10|\n",
      "|      {'NORM': 100.0}|22.0|  1| 67.41573|  0.3258427| 0.09948739|     273.0|0.23525026|     156.0| 0.10430878|      42.0|   1|  0|   0|  0|  0|         9|\n",
      "|       {'NORM': 80.0}|20.0|  1|47.244095| 0.11023622|0.041416623|     310.0|0.26207286| 181.66667|0.046765503|      50.0|   1|  0|   0|  0|  0|         5|\n",
      "|       {'NORM': 80.0}|43.0|  1| 78.75895|0.052505966| 0.01565021|     270.0|0.23221219| 219.09091| 0.17888726|      40.0|   1|  0|   0|  0|  0|         8|\n",
      "|      {'NORM': 100.0}|58.0|  1|75.093864|  0.2252816| 0.07744545|     263.0| 0.2188103|     130.0| 0.06835515|      73.0|   1|  0|   0|  0|  0|         2|\n",
      "|      {'NORM': 100.0}|19.0|  1| 60.74241| 0.30371204|0.089254595|     270.0|0.16920877| 137.77777|0.105735324| 78.888885|   1|  0|   0|  0|  0|         7|\n",
      "|      {'NORM': 100.0}|17.0|  1| 82.19178|  0.1369863| 0.04745345| 253.33333|0.19107272|     165.0| 0.14520526| 95.833336|   1|  0|   0|  0|  0|         3|\n",
      "|      {'NORM': 100.0}|49.0|  0|150.49074| 0.27589968| 0.10827422| 142.60869|0.43846026|  256.9565|  0.4270125|118.695656|   1|  0|   0|  0|  0|         6|\n",
      "|                   {}|56.0|  0|128.08989| 0.42696628| 0.12757634| 128.94737|0.37667903| 212.10527|  0.3422627| 107.89474|   0|  0|   0|  0|  0|         9|\n",
      "|                   {}|56.0|  0|56.737587| 0.22695035| 0.07664097|     247.5|0.23936589|     172.5|0.033864316|     53.75|   0|  0|   0|  0|  0|         9|\n",
      "|      {'NORM': 100.0}|20.0|  0|139.68958|0.046563193|0.013395838|     140.0|0.33492428|  289.5238| 0.33617455| 142.38095|   1|  0|   0|  0|  0|         7|\n",
      "|                   {}|56.0|  0| 72.11539| 0.21634616| 0.06921241|     230.0|0.23347634|     202.0| 0.03198142|      83.0|   0|  0|   0|  0|  0|         9|\n",
      "+---------------------+----+---+---------+-----------+-----------+----------+----------+----------+-----------+----------+----+---+----+---+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# retain these predictors for Part 1\n",
    "vars_to_keep = [\"diagnostic_superclass\",\"age\",\"sex\", \"bpm\", \"bif\", \"bif2\", \"TRinterval\", \"TRratio\",\"PRinterval\",\"PRratio\",\"QRinterval\",\"NORM\", \"MI\",\"STTC\",\"HYP\",\"CD\",\n",
    "               \"strat_fold\"]\n",
    "\n",
    "# subset the dataframe on these predictors\n",
    "df2 = df[vars_to_keep]\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----+---+---------+-----------+-----------+----------+----------+----------+-----------+----------+----+---+----+---+---+----------+--------------------+\n",
      "|diagnostic_superclass| age|sex|      bpm|        bif|       bif2|TRinterval|   TRratio|PRinterval|    PRratio|QRinterval|NORM| MI|STTC|HYP| CD|strat_fold|            features|\n",
      "+---------------------+----+---+---------+-----------+-----------+----------+----------+----------+-----------+----------+----+---+----+---+---+----------+--------------------+\n",
      "|      {'NORM': 100.0}|56.0|  1|47.244095| 0.16535433|0.058571953|     285.0|0.22329463| 151.66667| 0.10876027|      45.0|   1|  0|   0|  0|  0|         3|[56.0,1.0,47.2440...|\n",
      "|       {'NORM': 80.0}|19.0|  0| 63.75443| 0.07438017|0.022087706|  478.8889|0.22907318| 168.88889| 0.17119157|  61.11111|   1|  0|   0|  0|  0|         2|[19.0,0.0,63.7544...|\n",
      "|      {'NORM': 100.0}|37.0|  1| 74.40812| 0.14881623|0.045572452| 231.81818|0.25935754| 204.54546| 0.09707572| 75.454544|   1|  0|   0|  0|  0|         5|[37.0,1.0,74.4081...|\n",
      "|      {'NORM': 100.0}|24.0|  0| 66.29834| 0.16574585| 0.05418873|     267.0|0.21640064|     138.0| 0.10672184|      47.0|   1|  0|   0|  0|  0|         3|[24.0,0.0,66.2983...|\n",
      "|      {'NORM': 100.0}|19.0|  1| 83.14088| 0.15242495|0.049262654|  568.3333|0.15125224| 163.33333| 0.15125224| 74.166664|   1|  0|   0|  0|  0|         4|[19.0,1.0,83.1408...|\n",
      "|      {'NORM': 100.0}|18.0|  1|61.997704|  0.0413318|0.013584569| 68.888885|0.12439417| 207.77777|0.044792965|     160.0|   1|  0|   0|  0|  0|         4|[18.0,1.0,61.9977...|\n",
      "|      {'NORM': 100.0}|54.0|  0| 73.74302|0.061452515| 0.01679698| 248.18182|0.30723533| 182.72728| 0.18345565|      50.0|   1|  0|   0|  0|  0|         7|[54.0,0.0,73.7430...|\n",
      "|         {'MI': 35.0}|48.0|  0|60.836502| 0.11153359| 0.03546532|     262.5|0.31097305|     170.0| 0.13649388|     46.25|   0|  1|   0|  0|  0|         9|[48.0,0.0,60.8365...|\n",
      "|      {'NORM': 100.0}|55.0|  0| 62.93706| 0.17832167|0.060963623| 234.44444|0.19047683|     180.0| 0.08553792| 68.888885|   1|  0|   0|  0|  0|        10|[55.0,0.0,62.9370...|\n",
      "|      {'NORM': 100.0}|22.0|  1| 67.41573|  0.3258427| 0.09948739|     273.0|0.23525026|     156.0| 0.10430878|      42.0|   1|  0|   0|  0|  0|         9|[22.0,1.0,67.4157...|\n",
      "|       {'NORM': 80.0}|20.0|  1|47.244095| 0.11023622|0.041416623|     310.0|0.26207286| 181.66667|0.046765503|      50.0|   1|  0|   0|  0|  0|         5|[20.0,1.0,47.2440...|\n",
      "|       {'NORM': 80.0}|43.0|  1| 78.75895|0.052505966| 0.01565021|     270.0|0.23221219| 219.09091| 0.17888726|      40.0|   1|  0|   0|  0|  0|         8|[43.0,1.0,78.7589...|\n",
      "|      {'NORM': 100.0}|58.0|  1|75.093864|  0.2252816| 0.07744545|     263.0| 0.2188103|     130.0| 0.06835515|      73.0|   1|  0|   0|  0|  0|         2|[58.0,1.0,75.0938...|\n",
      "|      {'NORM': 100.0}|19.0|  1| 60.74241| 0.30371204|0.089254595|     270.0|0.16920877| 137.77777|0.105735324| 78.888885|   1|  0|   0|  0|  0|         7|[19.0,1.0,60.7424...|\n",
      "|      {'NORM': 100.0}|17.0|  1| 82.19178|  0.1369863| 0.04745345| 253.33333|0.19107272|     165.0| 0.14520526| 95.833336|   1|  0|   0|  0|  0|         3|[17.0,1.0,82.1917...|\n",
      "|      {'NORM': 100.0}|49.0|  0|150.49074| 0.27589968| 0.10827422| 142.60869|0.43846026|  256.9565|  0.4270125|118.695656|   1|  0|   0|  0|  0|         6|[49.0,0.0,150.490...|\n",
      "|                   {}|56.0|  0|128.08989| 0.42696628| 0.12757634| 128.94737|0.37667903| 212.10527|  0.3422627| 107.89474|   0|  0|   0|  0|  0|         9|[56.0,0.0,128.089...|\n",
      "|                   {}|56.0|  0|56.737587| 0.22695035| 0.07664097|     247.5|0.23936589|     172.5|0.033864316|     53.75|   0|  0|   0|  0|  0|         9|[56.0,0.0,56.7375...|\n",
      "|      {'NORM': 100.0}|20.0|  0|139.68958|0.046563193|0.013395838|     140.0|0.33492428|  289.5238| 0.33617455| 142.38095|   1|  0|   0|  0|  0|         7|[20.0,0.0,139.689...|\n",
      "|                   {}|56.0|  0| 72.11539| 0.21634616| 0.06921241|     230.0|0.23347634|     202.0| 0.03198142|      83.0|   0|  0|   0|  0|  0|         9|[56.0,0.0,72.1153...|\n",
      "+---------------------+----+---+---------+-----------+-----------+----------+----------+----------+-----------+----------+----+---+----+---+---+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Vector Assembler is a useful function for this, \n",
    "# here is a useful tutorial: https://spark.apache.org/docs/latest/ml-features#vectorassembler\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"age\",\"sex\", \"bpm\", \"bif\", \"bif2\", \"TRinterval\", \"TRratio\",\"PRinterval\",\"PRratio\",\"QRinterval\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "df2 = assembler.transform(df2)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.mllib.regression as reg\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----+---+---------+-----------+-----------+----------+----------+----------+-----------+----------+----+---+----+---+---+----------+--------------------+--------------------+\n",
      "|diagnostic_superclass| age|sex|      bpm|        bif|       bif2|TRinterval|   TRratio|PRinterval|    PRratio|QRinterval|NORM| MI|STTC|HYP| CD|strat_fold|            features|     features_scaled|\n",
      "+---------------------+----+---+---------+-----------+-----------+----------+----------+----------+-----------+----------+----+---+----+---+---+----------+--------------------+--------------------+\n",
      "|      {'NORM': 100.0}|56.0|  1|47.244095| 0.16535433|0.058571953|     285.0|0.22329463| 151.66667| 0.10876027|      45.0|   1|  0|   0|  0|  0|         3|[56.0,1.0,47.2440...|[3.22901707019790...|\n",
      "|       {'NORM': 80.0}|19.0|  0| 63.75443| 0.07438017|0.022087706|  478.8889|0.22907318| 168.88889| 0.17119157|  61.11111|   1|  0|   0|  0|  0|         2|[19.0,0.0,63.7544...|[1.09555936310286...|\n",
      "|      {'NORM': 100.0}|37.0|  1| 74.40812| 0.14881623|0.045572452| 231.81818|0.25935754| 204.54546| 0.09707572| 75.454544|   1|  0|   0|  0|  0|         5|[37.0,1.0,74.4081...|[2.13345770709504...|\n",
      "|      {'NORM': 100.0}|24.0|  0| 66.29834| 0.16574585| 0.05418873|     267.0|0.21640064|     138.0| 0.10672184|      47.0|   1|  0|   0|  0|  0|         3|[24.0,0.0,66.2983...|[1.38386445865624...|\n",
      "|      {'NORM': 100.0}|19.0|  1| 83.14088| 0.15242495|0.049262654|  568.3333|0.15125224| 163.33333| 0.15125224| 74.166664|   1|  0|   0|  0|  0|         4|[19.0,1.0,83.1408...|[1.09555936310286...|\n",
      "|      {'NORM': 100.0}|18.0|  1|61.997704|  0.0413318|0.013584569| 68.888885|0.12439417| 207.77777|0.044792965|     160.0|   1|  0|   0|  0|  0|         4|[18.0,1.0,61.9977...|[1.03789834399218...|\n",
      "|      {'NORM': 100.0}|54.0|  0| 73.74302|0.061452515| 0.01679698| 248.18182|0.30723533| 182.72728| 0.18345565|      50.0|   1|  0|   0|  0|  0|         7|[54.0,0.0,73.7430...|[3.11369503197655...|\n",
      "|         {'MI': 35.0}|48.0|  0|60.836502| 0.11153359| 0.03546532|     262.5|0.31097305|     170.0| 0.13649388|     46.25|   0|  1|   0|  0|  0|         9|[48.0,0.0,60.8365...|[2.76772891731249...|\n",
      "|      {'NORM': 100.0}|55.0|  0| 62.93706| 0.17832167|0.060963623| 234.44444|0.19047683|     180.0| 0.08553792| 68.888885|   1|  0|   0|  0|  0|        10|[55.0,0.0,62.9370...|[3.17135605108722...|\n",
      "|      {'NORM': 100.0}|22.0|  1| 67.41573|  0.3258427| 0.09948739|     273.0|0.23525026|     156.0| 0.10430878|      42.0|   1|  0|   0|  0|  0|         9|[22.0,1.0,67.4157...|[1.26854242043489...|\n",
      "|       {'NORM': 80.0}|20.0|  1|47.244095| 0.11023622|0.041416623|     310.0|0.26207286| 181.66667|0.046765503|      50.0|   1|  0|   0|  0|  0|         5|[20.0,1.0,47.2440...|[1.15322038221353...|\n",
      "|       {'NORM': 80.0}|43.0|  1| 78.75895|0.052505966| 0.01565021|     270.0|0.23221219| 219.09091| 0.17888726|      40.0|   1|  0|   0|  0|  0|         8|[43.0,1.0,78.7589...|[2.47942382175910...|\n",
      "|      {'NORM': 100.0}|58.0|  1|75.093864|  0.2252816| 0.07744545|     263.0| 0.2188103|     130.0| 0.06835515|      73.0|   1|  0|   0|  0|  0|         2|[58.0,1.0,75.0938...|[3.34433910841926...|\n",
      "|      {'NORM': 100.0}|19.0|  1| 60.74241| 0.30371204|0.089254595|     270.0|0.16920877| 137.77777|0.105735324| 78.888885|   1|  0|   0|  0|  0|         7|[19.0,1.0,60.7424...|[1.09555936310286...|\n",
      "|      {'NORM': 100.0}|17.0|  1| 82.19178|  0.1369863| 0.04745345| 253.33333|0.19107272|     165.0| 0.14520526| 95.833336|   1|  0|   0|  0|  0|         3|[17.0,1.0,82.1917...|[0.98023732488150...|\n",
      "|      {'NORM': 100.0}|49.0|  0|150.49074| 0.27589968| 0.10827422| 142.60869|0.43846026|  256.9565|  0.4270125|118.695656|   1|  0|   0|  0|  0|         6|[49.0,0.0,150.490...|[2.82538993642316...|\n",
      "|                   {}|56.0|  0|128.08989| 0.42696628| 0.12757634| 128.94737|0.37667903| 212.10527|  0.3422627| 107.89474|   0|  0|   0|  0|  0|         9|[56.0,0.0,128.089...|[3.22901707019790...|\n",
      "|                   {}|56.0|  0|56.737587| 0.22695035| 0.07664097|     247.5|0.23936589|     172.5|0.033864316|     53.75|   0|  0|   0|  0|  0|         9|[56.0,0.0,56.7375...|[3.22901707019790...|\n",
      "|      {'NORM': 100.0}|20.0|  0|139.68958|0.046563193|0.013395838|     140.0|0.33492428|  289.5238| 0.33617455| 142.38095|   1|  0|   0|  0|  0|         7|[20.0,0.0,139.689...|[1.15322038221353...|\n",
      "|                   {}|56.0|  0| 72.11539| 0.21634616| 0.06921241|     230.0|0.23347634|     202.0| 0.03198142|      83.0|   0|  0|   0|  0|  0|         9|[56.0,0.0,72.1153...|[3.22901707019790...|\n",
      "+---------------------+----+---+---------+-----------+-----------+----------+----------+----------+-----------+----------+----+---+----+---+---+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Feature scaling\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Initialize the `standardScaler`\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\", \n",
    "                                withStd=True, withMean=False)\n",
    "\n",
    "# Fit the DataFrame to the scaler; this computes the mean, standard deviation of each feature\n",
    "scaler = standardScaler.fit(df2)\n",
    "\n",
    "# Transform the data in `df2` with the scaler\n",
    "scaled_df = scaler.transform(df2)\n",
    "\n",
    "scaled_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features_scaled                                                                                                                                                                                |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[3.2290170701979064,2.0017353669023907,2.7237711054696416,0.6134522334681485,0.731419493527598,1.3959372577100275,1.1713254574007597,2.099795681158586,0.7921416087791956,0.7367600389800208]  |\n",
      "|[1.095559363102861,0.0,3.6756439453557546,0.2759448783056445,0.27582107140359,2.3456099563858066,1.2016377134218348,2.3382338932194275,1.2468521021593948,1.0005383176012221]                  |\n",
      "|[2.133457707095045,2.0017353669023907,4.289862801756859,0.5520971201361058,0.5690877296789019,1.1354513307685237,1.3604988637606141,2.831892201522032,0.707038712292621,1.2353753961845482]    |\n",
      "|[1.3838644586562456,0.0,3.822308438453208,0.6149047732214307,0.6766838048060877,1.3077727993283415,1.1351619867163765,1.9105832590032255,0.777294986565811,0.7695049296013551]                 |\n",
      "|[1.095559363102861,2.0017353669023907,4.793333824156115,0.5654851970088215,0.615169264788103,2.7837110399933898,0.7934162976534224,2.2613182796096463,1.1016264987012414,1.2142896522371973]   |\n",
      "|[1.0378983439921843,2.0017353669023907,3.57436318934931,0.15333790496693273,0.16963782295290414,0.3374195154696268,0.6525282706180578,2.876642977232918,0.3262438776324346,2.619591249706741]  |\n",
      "|[3.1136950319765524,0.0,4.251517682984972,0.22798425458128913,0.20975292687983377,1.2156008927431017,1.611648980549663,2.5298237788948734,1.336175921021994,0.8186222655333565]                |\n",
      "|[2.7677289173124913,0.0,3.50741626104106,0.4137813094307593,0.4428745154493436,1.28573168473292,1.6312557399123893,2.353617058192379,0.9941358316687238,0.7572255956183548]                    |\n",
      "|[3.1713560510872294,0.0,3.6285201273954515,0.6615601289680603,0.7612855623033197,1.1483148508646572,0.9991747896926712,2.4920651204389896,0.6230045720406454,1.1278795103295824]               |\n",
      "|[1.2685424204348918,2.0017353669023907,3.8867296983207384,1.2088521757334514,1.2423525223048792,1.3371609521222367,1.2340405264613208,2.1597897710471248,0.7597197393336735,0.6876427030480194]|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaled_df.select('features_scaled').show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# according to the documentation, we should reserve \"strat_fold\"'s #9 and #10 for testing\n",
    "# seed = 314\n",
    "train_data = scaled_df[scaled_df.strat_fold < 9]\n",
    "test_data = scaled_df[scaled_df.strat_fold > 8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17441\n",
      "4396\n"
     ]
    }
   ],
   "source": [
    "print(train_data.count())\n",
    "print(test_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a few different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "maxIter=10\n",
    "regParam=0.3\n",
    "elasticNetParam=0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_col(df, col): \n",
    "    return df.select(F.sum(col)).collect()[0][0]\n",
    "\n",
    "def getpredictions(model, test_dat, column):\n",
    "    predictions = model.transform(test_dat)\n",
    "    predictions = predictions.withColumn('TP', F.when((F.col(column) == 1) & (F.col(\"prediction\")==1), 1).otherwise(0))\n",
    "    predictions = predictions.withColumn('TN', F.when((F.col(column) == 0) & (F.col(\"prediction\")==0), 1).otherwise(0))\n",
    "    predictions = predictions.withColumn('FP', F.when((F.col(column) == 0) & (F.col(\"prediction\")==1), 1).otherwise(0))\n",
    "    predictions = predictions.withColumn('FN', F.when((F.col(column) == 1) & (F.col(\"prediction\")==0), 1).otherwise(0))\n",
    "    accuracy = (sum_col(predictions, \"TP\")+sum_col(predictions, \"TN\"))/predictions.count()\n",
    "    if (sum_col(predictions, \"TP\")+sum_col(predictions, \"FP\"))>0:\n",
    "        precision = sum_col(predictions, \"TP\")/(sum_col(predictions, \"TP\")+sum_col(predictions, \"FP\"))\n",
    "    else:\n",
    "        precision = \"Precision could not be calculated due to a lack of TP/FP values\"\n",
    "    if (sum_col(predictions, \"TP\")+sum_col(predictions, \"FN\"))>0:\n",
    "        recall = sum_col(predictions, \"TP\")/(sum_col(predictions, \"TP\")+sum_col(predictions, \"FN\"))\n",
    "    else:\n",
    "        precision = \"Recall could not be calculated due to a lack of TP/FN values\"\n",
    "    if ((2*sum_col(predictions, \"TP\")+sum_col(predictions, \"FP\")+sum_col(predictions, \"FN\")))>0:\n",
    "        fmeasure = (2*sum_col(predictions, \"TP\"))/(2*sum_col(predictions, \"TP\")+sum_col(predictions, \"FP\")+sum_col(predictions, \"FN\"))\n",
    "    else:\n",
    "        fmeasure = \"Fmeasure could not be calculated due to a lack of TP/FN/FP values\"\n",
    "    print(\"accuracy:\"+str(accuracy))\n",
    "    print(\"precision:\"+str(precision))\n",
    "    print(\"recall:\"+str(recall))\n",
    "    print(\"fmeasure:\"+str(fmeasure))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Model with only 2 variables, age and sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this benchmark are for training data:\n",
      "accuracy:0.5636763291660942\n",
      "precision:Precision could not be calculated due to a lack of TP/FP values\n",
      "recall:0.0\n",
      "fmeasure:0.0\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"age\",\"sex\"],\n",
    "    outputCol=\"ageandsex\")\n",
    "\n",
    "df_bm = assembler.transform(df)\n",
    "\n",
    "model1 = LogisticRegression(featuresCol=\"ageandsex\", labelCol=\"NORM\",maxIter=3, regParam=1, elasticNetParam=1)\n",
    "model_bm = model1.fit(df_bm)\n",
    "print(\"this benchmark are for training data:\")\n",
    "getpredictions(model_bm, df_bm, \"NORM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "maxIter=10\n",
    "regParam=0.3\n",
    "elasticNetParam=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.6831210191082803\n",
      "precision:0.6924198250728864\n",
      "recall:0.49453409682457056\n",
      "fmeasure:0.5769814758578804\n"
     ]
    }
   ],
   "source": [
    "lrNORM = LogisticRegression(featuresCol=\"features_scaled\", labelCol=\"NORM\",maxIter=100, )\n",
    "modelLR_NORM = lrNORM.fit(train_data)\n",
    "getpredictions(modelLR_NORM, test_data, \"NORM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.7659235668789809\n",
      "precision:Precision could not be calculated due to a lack of TP/FP values\n",
      "recall:0.0\n",
      "fmeasure:0.0\n"
     ]
    }
   ],
   "source": [
    "lrSTTC = LogisticRegression(featuresCol=\"features_scaled\", labelCol=\"STTC\",maxIter=20, regParam=0.0, elasticNetParam=0)\n",
    "modelLR_STTC = lrSTTC.fit(train_data)\n",
    "getpredictions(modelLR_STTC, test_data, \"STTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.7538671519563239\n",
      "precision:0.5773195876288659\n",
      "recall:0.05104831358249772\n",
      "fmeasure:0.09380234505862646\n"
     ]
    }
   ],
   "source": [
    "lrMI = LogisticRegression(featuresCol=\"features_scaled\", labelCol=\"MI\",maxIter=20, regParam=0.0, elasticNetParam=0.)\n",
    "modelLR_MI = lrMI.fit(train_data)\n",
    "getpredictions(modelLR_MI, test_data, \"MI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.87852593266606\n",
      "precision:Precision could not be calculated due to a lack of TP/FP values\n",
      "recall:0.0\n",
      "fmeasure:0.0\n"
     ]
    }
   ],
   "source": [
    "lrHYP = LogisticRegression(featuresCol=\"features_scaled\", labelCol=\"HYP\",maxIter=20, regParam=0.01, elasticNetParam=0.01)\n",
    "modelLR_HYP = lrHYP.fit(train_data)\n",
    "getpredictions(modelLR_HYP, test_data, \"HYP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.7734303912647862\n",
      "precision:0.0\n",
      "recall:0.0\n",
      "fmeasure:0.0\n"
     ]
    }
   ],
   "source": [
    "lrCD = LogisticRegression(featuresCol=\"features_scaled\", labelCol=\"CD\",maxIter=20, regParam=0.01, elasticNetParam=0.01)\n",
    "modelLR_CD = lrCD.fit(train_data)\n",
    "getpredictions(modelLR_CD, test_data, \"CD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.6762966333030027\n",
      "precision:0.6828193832599119\n",
      "recall:0.4841228526808954\n",
      "fmeasure:0.5665549802010357\n"
     ]
    }
   ],
   "source": [
    "rfNORM = RandomForestClassifier(featuresCol=\"features_scaled\", labelCol=\"NORM\",maxDepth=10, numTrees=10, seed=10)\n",
    "modelrf_NORM = rfNORM.fit(train_data)\n",
    "getpredictions(modelrf_NORM, test_data, \"NORM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.7859067714007224\n",
      "precision:0.9522613065326633\n",
      "recall:0.09257449926722032\n",
      "fmeasure:0.16874443455031166\n"
     ]
    }
   ],
   "source": [
    "rfSTTC = RandomForestClassifier(featuresCol=\"features_scaled\", labelCol=\"STTC\",maxDepth=10, numTrees=10, seed=10)\n",
    "modelrf_STTC = rfSTTC.fit(train_data)\n",
    "getpredictions(modelrf_STTC, train_data, \"STTC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.6603730664240218\n",
      "precision:0.6453804347826086\n",
      "recall:0.49453409682457056\n",
      "fmeasure:0.5599764220453876\n"
     ]
    }
   ],
   "source": [
    "gbtNORM = GBTClassifier(featuresCol=\"features_scaled\", labelCol=\"NORM\", maxDepth=10, maxIter=10, seed=10)\n",
    "modelgbt_NORM = gbtNORM.fit(train_data)\n",
    "getpredictions(modelgbt_NORM, test_data, \"NORM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.6364877161055505\n",
      "precision:0.6551392891450528\n",
      "recall:0.3550234252993233\n",
      "fmeasure:0.46049966239027684\n"
     ]
    }
   ],
   "source": [
    "svmNORM = LinearSVC(featuresCol=\"features_scaled\", labelCol=\"NORM\", maxIter=20, regParam=0.001)\n",
    "modelsvm_NORM = svmNORM.fit(train_data)\n",
    "getpredictions(modelsvm_NORM, test_data, \"NORM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline and Cross Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import LinearSVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with df table loaded from .csv file\n",
    "vars_to_keep = [\"diagnostic_superclass\",\"age\",\"sex\", \"bpm\", \"bif\", \"bif2\", \"TRinterval\", \"TRratio\",\"PRinterval\",\"PRratio\",\"QRinterval\",\"NORM\", \"MI\",\"STTC\",\"HYP\",\"CD\",\n",
    "               \"strat_fold\"]\n",
    "\n",
    "# subset the dataframe on these predictors\n",
    "df2 = df[vars_to_keep]\n",
    "\n",
    "#re-define train_data and test_data, assembler and standardScaler will be part of pipeline\n",
    "train_data = df2[df2.strat_fold < 9]\n",
    "test_data = df2[df2.strat_fold > 8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(paramGrid): 36\n",
      "ROC for test is 0.7585495769774779\n",
      "best regParam is: 0.01\n",
      "best elasticNetParam is: 0.2\n"
     ]
    }
   ],
   "source": [
    "train=train_data.withColumnRenamed(\"NORM\",\"label\")\n",
    "test=test_data.withColumnRenamed(\"NORM\",\"label\")\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: assembler, standardScaler, and lr.\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"age\",\"sex\", \"bpm\", \"bif\", \"bif2\", \"TRinterval\", \"TRratio\",\"PRinterval\",\"PRratio\",\"QRinterval\"],\n",
    "    outputCol=\"features\")\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\", \n",
    "                                withStd=True, withMean=False)\n",
    "lr = LogisticRegression(featuresCol=\"features_scaled\", maxIter=100)\n",
    "pipeline = Pipeline(stages=[assembler, standardScaler, lr])\n",
    "\n",
    "# Set up the parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.elasticNetParam, [0, 0.2, 0.4, 0.6, 0.8, 1.0]) \\\n",
    "    .addGrid(lr.regParam, [0, 0.01, 0.05, 1, 2, 5]) \\\n",
    "    .build()\n",
    "\n",
    "print('len(paramGrid): {}'.format(len(paramGrid)))\n",
    "\n",
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=10,\n",
    "                          parallelism=8)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "bestModel_lr=cvModel.bestModel\n",
    "\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = bestModel_lr.transform(test)\n",
    "\n",
    "evaluator=BinaryClassificationEvaluator()\n",
    "print(\"ROC for test is {}\".format(evaluator.evaluate(prediction)))\n",
    "\n",
    "best_reg = bestModel_lr.stages[2]._java_obj.getRegParam()\n",
    "best_elastic_net = bestModel_lr.stages[2]._java_obj.getElasticNetParam()\n",
    "\n",
    "print(\"best regParam is:\", best_reg)\n",
    "print(\"best elasticNetParam is:\", best_elastic_net )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.6808462238398544\n",
      "precision:0.6938622754491018\n",
      "recall:0.48256116605934407\n",
      "fmeasure:0.5692354927847713\n"
     ]
    }
   ],
   "source": [
    "getpredictions(bestModel_lr, test, \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(paramGrid): 36\n",
      "ROC for test is 0.6895960019928093\n",
      "best regParam is: 0.01\n",
      "best elasticNetParam is: 0.4\n"
     ]
    }
   ],
   "source": [
    "#for MI\n",
    "\n",
    "train=train_data.withColumnRenamed(\"MI\",\"label\")\n",
    "test=test_data.withColumnRenamed(\"MI\",\"label\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"age\",\"sex\", \"bpm\", \"bif\", \"bif2\", \"TRinterval\", \"TRratio\",\"PRinterval\",\"PRratio\",\"QRinterval\"],\n",
    "    outputCol=\"features\")\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\", \n",
    "                                withStd=True, withMean=False)\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features_scaled\", maxIter=100)\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, standardScaler, lr])\n",
    "\n",
    "# Set up the parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.elasticNetParam, [0, 0.2, 0.4, 0.6, 0.8, 1.0]) \\\n",
    "    .addGrid(lr.regParam, [0, 0.01, 0.05, 1, 2, 5]) \\\n",
    "    .build()\n",
    "\n",
    "print('len(paramGrid): {}'.format(len(paramGrid)))\n",
    "\n",
    "# Treat the lr as an Estimator, wrapping it in a CrossValidator instance.\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=10,\n",
    "                          parallelism=8)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "bestModel_lr_MI=cvModel.bestModel\n",
    "\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = bestModel_lr_MI.transform(test)\n",
    "\n",
    "evaluator=BinaryClassificationEvaluator()\n",
    "print(\"ROC for test is {}\".format(evaluator.evaluate(prediction)))\n",
    "\n",
    "best_reg = bestModel_lr_MI.stages[2]._java_obj.getRegParam()\n",
    "best_elastic_net = bestModel_lr_MI.stages[2]._java_obj.getElasticNetParam()\n",
    "\n",
    "print(\"best regParam is:\", best_reg)\n",
    "print(\"best elasticNetParam is:\", best_elastic_net )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.7511373976342129\n",
      "precision:0.5405405405405406\n",
      "recall:0.018231540565177756\n",
      "fmeasure:0.03527336860670194\n"
     ]
    }
   ],
   "source": [
    "getpredictions(bestModel_lr_MI, test, \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(paramGrid): 12\n",
      "ROC for test is 0.7581011573307237\n",
      "best maxDepth is: 7\n",
      "best numTrees is: 200\n"
     ]
    }
   ],
   "source": [
    "#for NORM\n",
    "\n",
    "train=train_data.withColumnRenamed(\"NORM\",\"label\")\n",
    "test=test_data.withColumnRenamed(\"NORM\",\"label\")\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: assembler, standardScaler, and rf.\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"age\",\"sex\", \"bpm\", \"bif\", \"bif2\", \"TRinterval\", \"TRratio\",\"PRinterval\",\"PRratio\",\"QRinterval\"],\n",
    "    outputCol=\"features\")\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\", \n",
    "                                withStd=True, withMean=False)\n",
    "rf = RandomForestClassifier(featuresCol=\"features_scaled\", seed=2020)\n",
    "pipeline = Pipeline(stages=[assembler, standardScaler, rf])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set up the parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [4, 7, 10, 12]) \\\n",
    "    .addGrid(rf.numTrees, [10,100,200]) \\\n",
    "    .build()\n",
    "\n",
    "print('len(paramGrid): {}'.format(len(paramGrid)))\n",
    "\n",
    "# Treat the pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=10,\n",
    "                          parallelism=10)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "bestModel_rf=cvModel.bestModel\n",
    "\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = bestModel_rf.transform(test)\n",
    "\n",
    "evaluator=BinaryClassificationEvaluator()\n",
    "print(\"ROC for test is {}\".format(evaluator.evaluate(prediction)))\n",
    "\n",
    "best_Depth = bestModel_rf.stages[2]._java_obj.getMaxDepth()\n",
    "best_numTree = bestModel_rf.stages[2]._java_obj.getNumTrees()\n",
    "\n",
    "print(\"best maxDepth is:\", best_Depth)\n",
    "print(\"best numTrees is:\", best_numTree )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.6867606915377616\n",
      "precision:0.7111801242236024\n",
      "recall:0.47683498178032274\n",
      "fmeasure:0.5708943596135868\n"
     ]
    }
   ],
   "source": [
    "getpredictions(bestModel_rf, test, \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(paramGrid): 16\n",
      "ROC for test is 0.756104512064948\n",
      "best maxDepth is: 4\n",
      "best stepSize is: 0.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "train=train_data.withColumnRenamed(\"NORM\",\"label\")\n",
    "test=test_data.withColumnRenamed(\"NORM\",\"label\")\n",
    "\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: assembler, standardScaler, and gbt.\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"age\",\"sex\", \"bpm\", \"bif\", \"bif2\", \"TRinterval\", \"TRratio\",\"PRinterval\",\"PRratio\",\"QRinterval\"],\n",
    "    outputCol=\"features\")\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\", \n",
    "                                withStd=True, withMean=False)\n",
    "gbt = GBTClassifier(featuresCol=\"features_scaled\", maxIter=10, seed=2020)\n",
    "pipeline = Pipeline(stages=[assembler, standardScaler, gbt])\n",
    "\n",
    "\n",
    "\n",
    "# Set up the parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [4,7,10,15]) \\\n",
    "    .addGrid(gbt.stepSize, [0.25,0.1,0.05,0.01]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "\n",
    "print('len(paramGrid): {}'.format(len(paramGrid)))\n",
    "\n",
    "# Treat the pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=10,\n",
    "                          parallelism=10)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "bestModel_gbt=cvModel.bestModel\n",
    "\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = bestModel_gbt.transform(test)\n",
    "\n",
    "evaluator=BinaryClassificationEvaluator()\n",
    "print(\"ROC for test is {}\".format(evaluator.evaluate(prediction)))\n",
    "\n",
    "best_Depth = bestModel_gbt.stages[2]._java_obj.getMaxDepth()\n",
    "best_step = bestModel_gbt.stages[2]._java_obj.getStepSize()\n",
    "\n",
    "print(\"best maxDepth is:\", best_Depth)\n",
    "print(\"best stepSize is:\", best_step )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.6792538671519563\n",
      "precision:0.7085714285714285\n",
      "recall:0.45184799583550234\n",
      "fmeasure:0.5518118245390973\n"
     ]
    }
   ],
   "source": [
    "getpredictions(bestModel_gbt, test, \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(paramGrid): 12\n",
      "ROC for test is 0.7518752333328067\n",
      "best maxIter is: 60\n",
      "best regParam is: 0.25\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "train=train_data.withColumnRenamed(\"NORM\",\"label\")\n",
    "test=test_data.withColumnRenamed(\"NORM\",\"label\")\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: assembler, standardScaler, and svm.\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"age\",\"sex\", \"bpm\", \"bif\", \"bif2\", \"TRinterval\", \"TRratio\",\"PRinterval\",\"PRratio\",\"QRinterval\"],\n",
    "    outputCol=\"features\")\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\", \n",
    "                                withStd=True, withMean=False)\n",
    "svm = LinearSVC(featuresCol=\"features_scaled\")\n",
    "pipeline = Pipeline(stages=[assembler, standardScaler, svm])\n",
    "\n",
    "\n",
    "# Set up the parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(svm.maxIter, [20,40,60]) \\\n",
    "    .addGrid(svm.regParam, [0.25,0.1,0.01,0.001]) \\\n",
    "    .build()\n",
    "\n",
    "print('len(paramGrid): {}'.format(len(paramGrid)))\n",
    "\n",
    "# Treat the pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=10,\n",
    "                          parallelism=10)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "bestModel_svm=cvModel.bestModel\n",
    "\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = bestModel_svm.transform(test)\n",
    "\n",
    "evaluator=BinaryClassificationEvaluator()\n",
    "print(\"ROC for test is {}\".format(evaluator.evaluate(prediction)))\n",
    "\n",
    "best_Iter = bestModel_svm.stages[2]._java_obj.getMaxIter()\n",
    "best_regParam = bestModel_svm.stages[2]._java_obj.getRegParam()\n",
    "\n",
    "print(\"best maxIter is:\", best_Iter)\n",
    "print(\"best regParam is:\", best_regParam )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.6530937215650592\n",
      "precision:0.7487437185929648\n",
      "recall:0.31025507548152004\n",
      "fmeasure:0.4387191755612808\n"
     ]
    }
   ],
   "source": [
    "getpredictions(bestModel_svm, test, \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use only three generated features: bpm, bif, bif2, plus age and sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(paramGrid): 36\n",
      "ROC for test is 0.7590557317053933\n",
      "best regParam is: 0.01\n",
      "best elasticNetParam is: 0.6\n"
     ]
    }
   ],
   "source": [
    "train=train_data.withColumnRenamed(\"NORM\",\"label\")\n",
    "test=test_data.withColumnRenamed(\"NORM\",\"label\")\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: assembler, standardScaler, and lr.\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"age\",\"sex\", \"bpm\", \"bif\", \"bif2\"],\n",
    "    outputCol=\"features2\")\n",
    "standardScaler = StandardScaler(inputCol=\"features2\", outputCol=\"features2_scaled\", \n",
    "                                withStd=True, withMean=False)\n",
    "lr = LogisticRegression(featuresCol=\"features2_scaled\", maxIter=100)\n",
    "pipeline = Pipeline(stages=[assembler, standardScaler, lr])\n",
    "\n",
    "# Set up the parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.elasticNetParam, [0, 0.2, 0.4, 0.6, 0.8, 1.0]) \\\n",
    "    .addGrid(lr.regParam, [0, 0.01, 0.05, 1, 2, 5]) \\\n",
    "    .build()\n",
    "\n",
    "print('len(paramGrid): {}'.format(len(paramGrid)))\n",
    "\n",
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=10,\n",
    "                          parallelism=8)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "bestModel_lr=cvModel.bestModel\n",
    "\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = bestModel_lr.transform(test)\n",
    "\n",
    "evaluator=BinaryClassificationEvaluator()\n",
    "print(\"ROC for test is {}\".format(evaluator.evaluate(prediction)))\n",
    "\n",
    "best_reg = bestModel_lr.stages[2]._java_obj.getRegParam()\n",
    "best_elastic_net = bestModel_lr.stages[2]._java_obj.getElasticNetParam()\n",
    "\n",
    "print(\"best regParam is:\", best_reg)\n",
    "print(\"best elasticNetParam is:\", best_elastic_net )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.6819836214740673\n",
      "precision:0.6955871353777113\n",
      "recall:0.4841228526808954\n",
      "fmeasure:0.570902394106814\n"
     ]
    }
   ],
   "source": [
    "getpredictions(bestModel_lr, test, \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(paramGrid): 12\n",
      "ROC for test is 0.7594174961483713\n",
      "best maxDepth is: 7\n",
      "best numTrees is: 200\n"
     ]
    }
   ],
   "source": [
    "train=train_data.withColumnRenamed(\"NORM\",\"label\")\n",
    "test=test_data.withColumnRenamed(\"NORM\",\"label\")\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: assembler, standardScaler, and rf.\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"age\",\"sex\", \"bpm\", \"bif\", \"bif2\"],\n",
    "    outputCol=\"features2\")\n",
    "standardScaler = StandardScaler(inputCol=\"features2\", outputCol=\"features2_scaled\", \n",
    "                                withStd=True, withMean=False)\n",
    "rf = RandomForestClassifier(featuresCol=\"features2_scaled\", seed=2020)\n",
    "pipeline = Pipeline(stages=[assembler, standardScaler, rf])\n",
    "\n",
    "# Set up the parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [4, 7, 10, 12]) \\\n",
    "    .addGrid(rf.numTrees, [10,100,200]) \\\n",
    "    .build()\n",
    "\n",
    "print('len(paramGrid): {}'.format(len(paramGrid)))\n",
    "\n",
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=10,\n",
    "                          parallelism=8)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "bestModel_rf=cvModel.bestModel\n",
    "\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = bestModel_rf.transform(test)\n",
    "\n",
    "evaluator=BinaryClassificationEvaluator()\n",
    "print(\"ROC for test is {}\".format(evaluator.evaluate(prediction)))\n",
    "\n",
    "best_Depth = bestModel_rf.stages[2]._java_obj.getMaxDepth()\n",
    "best_numTree = bestModel_rf.stages[2]._java_obj.getNumTrees()\n",
    "\n",
    "print(\"best maxDepth is:\", best_Depth)\n",
    "print(\"best numTrees is:\", best_numTree )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.6813011828935396\n",
      "precision:0.708\n",
      "recall:0.46069755335762624\n",
      "fmeasure:0.5581835383159887\n"
     ]
    }
   ],
   "source": [
    "getpredictions(bestModel_rf, test, \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(paramGrid): 16\n",
      "ROC for test is 0.7587829360760141\n",
      "best maxDepth is: 4\n",
      "best stepSize is: 0.1\n"
     ]
    }
   ],
   "source": [
    "train=train_data.withColumnRenamed(\"NORM\",\"label\")\n",
    "test=test_data.withColumnRenamed(\"NORM\",\"label\")\n",
    "\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: assembler, standardScaler, and gbt.\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"age\",\"sex\", \"bpm\", \"bif\", \"bif2\"],\n",
    "    outputCol=\"features2\")\n",
    "standardScaler = StandardScaler(inputCol=\"features2\", outputCol=\"features2_scaled\", \n",
    "                                withStd=True, withMean=False)\n",
    "gbt = GBTClassifier(featuresCol=\"features2_scaled\", maxIter=10, seed=2020)\n",
    "pipeline = Pipeline(stages=[assembler, standardScaler, gbt])\n",
    "\n",
    "\n",
    "\n",
    "# Set up the parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [4,7,10,15]) \\\n",
    "    .addGrid(gbt.stepSize, [0.25,0.1,0.05,0.01]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "\n",
    "print('len(paramGrid): {}'.format(len(paramGrid)))\n",
    "\n",
    "# Treat the pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=10,\n",
    "                          parallelism=10)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "bestModel_gbt=cvModel.bestModel\n",
    "\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = bestModel_gbt.transform(test)\n",
    "\n",
    "evaluator=BinaryClassificationEvaluator()\n",
    "print(\"ROC for test is {}\".format(evaluator.evaluate(prediction)))\n",
    "\n",
    "best_Depth = bestModel_gbt.stages[2]._java_obj.getMaxDepth()\n",
    "best_step = bestModel_gbt.stages[2]._java_obj.getStepSize()\n",
    "\n",
    "print(\"best maxDepth is:\", best_Depth)\n",
    "print(\"best stepSize is:\", best_step )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.6799363057324841\n",
      "precision:0.7086038961038961\n",
      "recall:0.45445080687142114\n",
      "fmeasure:0.5537583254043768\n"
     ]
    }
   ],
   "source": [
    "getpredictions(bestModel_gbt, test, \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(paramGrid): 12\n",
      "ROC for test is 0.7590226050194803\n",
      "best maxIter is: 60\n",
      "best regParam is: 0.25\n"
     ]
    }
   ],
   "source": [
    "train=train_data.withColumnRenamed(\"NORM\",\"label\")\n",
    "test=test_data.withColumnRenamed(\"NORM\",\"label\")\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: assembler, standardScaler, and svm.\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"age\",\"sex\", \"bpm\", \"bif\", \"bif2\"],\n",
    "    outputCol=\"features2\")\n",
    "standardScaler = StandardScaler(inputCol=\"features2\", outputCol=\"features2_scaled\", \n",
    "                                withStd=True, withMean=False)\n",
    "svm = LinearSVC(featuresCol=\"features2_scaled\")\n",
    "pipeline = Pipeline(stages=[assembler, standardScaler, svm])\n",
    "\n",
    "\n",
    "# Set up the parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(svm.maxIter, [20,40,60]) \\\n",
    "    .addGrid(svm.regParam, [0.25,0.1,0.01,0.001]) \\\n",
    "    .build()\n",
    "\n",
    "print('len(paramGrid): {}'.format(len(paramGrid)))\n",
    "\n",
    "# Treat the pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=10,\n",
    "                          parallelism=10)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "bestModel_svm=cvModel.bestModel\n",
    "\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = bestModel_svm.transform(test)\n",
    "\n",
    "evaluator=BinaryClassificationEvaluator()\n",
    "print(\"ROC for test is {}\".format(evaluator.evaluate(prediction)))\n",
    "\n",
    "best_Iter = bestModel_svm.stages[2]._java_obj.getMaxIter()\n",
    "best_regParam = bestModel_svm.stages[2]._java_obj.getRegParam()\n",
    "\n",
    "print(\"best maxIter is:\", best_Iter)\n",
    "print(\"best regParam is:\", best_regParam )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.6599181073703366\n",
      "precision:0.75177304964539\n",
      "recall:0.3310775637688704\n",
      "fmeasure:0.459703650162631\n"
     ]
    }
   ],
   "source": [
    "getpredictions(bestModel_svm, test, \"label\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
